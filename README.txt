The only executable in the folder is called somMapAnimalData. Run it and take a look at udm10x10_iter10000.png.

----------------

Notes on the algorithm:

I think that the quantization side of this algorithm was covered in enough detail in my comments to the counterpropagation code and in in-class discussions of the Kohonen review paper. Here I will focus on the self-organization aspect.

Dynamical nature of the self-organizational process makes video the best medium to illustrate my ideas. The videos referred to here are all stored in a public Youtube playlist https://www.youtube.com/playlist?list=PL2o0um7m72J00X5ic8NJ4cBQILrQmdRZo , under the same numbers as those which are used in text.

Self-organization of the Kohonen map can be described as a process of optimization of positions of the best-matching units (BMUs) on two-dimensional grid of Kohonen vectors. Initially these positions are random, since they determined by the random initial values of Kohonen vectors. Over the course of the learining process they move on the grid to an optimal (possibly locally optimal) combination of positions, in which "forces" acting upon them are at equilibrium. I found three forces influencing the movement of BMUs:

1. Repulsion. If the neighborhood of the BMU of vector A includes BMU of a different vector B=/=A, and learning rule is applied to the neighbors of the BMU of A, then BMU of B will most likely move out of the A's BMU's neighborhood. This increases the distance between the two BMUs, effectively acting as a repulsive force. This force acts only within neighborhoods of BMUs and is governed by their radii. The amount of difference between the two vectors does not matter as long as the alpha constant is high. Due to these properties, this force dominates the initial stage of the self-organization (see video 1).

2. Attraction. Suppose there are two vectors A and B among the training patterns which are similar. When the learning rule is applied to the neighborhood of the BMU of vector A, the vectors of this neighborhood are updated to be essentially a set of noisified copies of A. Since A and B are similar, it is likely that one of the vectors of the aforementioned set is similar to B moreso than B's current BMU. This causes B's BMU to "teleport" into the vicinity of A, acting as a long range attractive force. This force is weak when alpha is close to 1 since there's no diversity in the updated neighborhood. For intermediate and low values of alpha the force works as described above. It requires, though, that there's some diversity in the neighborhood, which at the later stages of the algorithm happens only during the late "untwisting events" (transitions of the system to a better optimal placement of BMUs). This force is also more influential when the diameter of the neighborhood is large, since there are more noisified versions of A which may turn out to be like B. For a video of the "teleportation" see video 1, the transition between 0:01 and 0:02, and observe the movement of the "owl, hawk" mark.

3. Gravity of the (0,0) angle. This is a relatively harmless floating point arithmetic artifact. When the grid is large enough for the neighborhoods of BMUs to not intersect at some point in the training history, algorithm converges exponentially, quickly leading all neighbors of the BMU of a vector to be exactly equal to the vector. This triggers the tiebreaking mechanism, which selects the lower left corner of the neighborhood to be the new BMU, because it corresponds to the distance minimum with smallest possible indices. Due to this BMUs tend to gather around the coordinate origin. This force becomes especially apparent when the diameter of the neighborhood is fixed at 1 and/or the size of the grid is many times the number of training patterns (see video 5 for size 1 neighborhood and video 3 for large grid).

The self-organization of the map is an interplay of these three forces. If the grid is not too big (e.g. 10x10 for our dataset, see video 2), repulsion dominates at the beginning, maintaining the learning process at a trajectory which is almost (see below) chaotic. These kinds of trajectories have a wonderful property of visiting if not all, then at least the majority of possible states of the network when they are followed for many iterations, effectively randomizing the state of the system. As neighborhood diameters decreases attraction becomes more prominent, and the states of the system where similar vectors are close and different are far away more probable. 

The method works very much like simulated annealing. Stability of the weights set with respect to the learning rule serves as a potential and the diameter of the neighborhood as a temperature.

On larger grids, chaotic exploration stage does not develop and the algorithm gets stuck in a local optimum (videos 4 and 5).

Random order of patterns' presentation enhances the chaotic exploration stage. I hypothesize that if the aren't many training patterns available (say, there's K of them, and K is a lot less than the number of Kohonen units), the system perhaps becomes prone to getting stuck on period-K orbits, which prevents it from exploring the rest of the possible states. This implies that the system's orbit at the early stage of the algorithm is not truly chaotic. However, when I tried to remove the randomization feature and observe what happens, the convergence was not affected drastically enough to be obvious without statistical tests. From this I conclude that the behavior of the system at the early stages of learning is at least close to being chaotic.

When the grid is very small (3x3), the balance of repulsion and attraction makes BMUs gather at its corners, effectively making the algorithm find exactly four optimal clusters (see udm3x3_iter10000.original.png).
